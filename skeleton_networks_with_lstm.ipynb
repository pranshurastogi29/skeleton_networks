{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skeleton_networks_with_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGlckYfgn6Sg61GU6qTOGP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshurastogi29/skeleton_networks/blob/master/skeleton_networks_with_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KGRcGaqB4QN",
        "colab_type": "code",
        "outputId": "13e703dc-84f6-434e-83d8-2e4ca4394582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO1wnjhrCEiI",
        "colab_type": "code",
        "outputId": "b81f5e2b-a8c4-4a5a-ffc2-00b9f63d086c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=59abdb84cc4e7a6062c62d64c010d9cbcae8f5474e96ed07c228800e0f547961\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJCRcibUBV1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.mkdir('SBU')\n",
        "SBU_dir ='/content/SBU/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSKFp1E7B0WT",
        "colab_type": "code",
        "outputId": "1fe161df-a320-49f3-bca1-d36cbf944fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "link = []\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s02.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s07.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s01.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s06.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s07.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s02.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s04.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s05.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s06.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s02.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s06.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s05s02.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s05s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s06s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s06s04.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s07s01.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s07s03.zip')\n",
        "\n",
        "import wget\n",
        "for i in link:\n",
        "    temp_path = SBU_dir + i.split('/')[-1]\n",
        "    url = i\n",
        "    print(url)\n",
        "    print(temp_path)\n",
        "    wget.download(url=url, out=temp_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s02.zip\n",
            "/content/SBU/s01s02.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s03.zip\n",
            "/content/SBU/s01s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s07.zip\n",
            "/content/SBU/s01s07.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s01.zip\n",
            "/content/SBU/s02s01.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s03.zip\n",
            "/content/SBU/s02s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s06.zip\n",
            "/content/SBU/s02s06.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s07.zip\n",
            "/content/SBU/s02s07.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s02.zip\n",
            "/content/SBU/s03s02.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s04.zip\n",
            "/content/SBU/s03s04.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s05.zip\n",
            "/content/SBU/s03s05.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s06.zip\n",
            "/content/SBU/s03s06.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s02.zip\n",
            "/content/SBU/s04s02.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s03.zip\n",
            "/content/SBU/s04s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s06.zip\n",
            "/content/SBU/s04s06.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s05s02.zip\n",
            "/content/SBU/s05s02.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s05s03.zip\n",
            "/content/SBU/s05s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s06s03.zip\n",
            "/content/SBU/s06s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s06s04.zip\n",
            "/content/SBU/s06s04.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s07s01.zip\n",
            "/content/SBU/s07s01.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s07s03.zip\n",
            "/content/SBU/s07s03.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8noWYUDB2BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/SBU/s07s01.zip'\n",
        "!unzip '/content/SBU/s07s03.zip'\n",
        "!unzip '/content/SBU/s06s04.zip'\n",
        "!unzip '/content/SBU/s06s03.zip'\n",
        "!unzip '/content/SBU/s06s02.zip'\n",
        "!unzip '/content/SBU/s05s03.zip'\n",
        "!unzip '/content/SBU/s05s02.zip'\n",
        "!unzip '/content/SBU/s04s02.zip'\n",
        "!unzip '/content/SBU/s04s03.zip'\n",
        "!unzip '/content/SBU/s04s06.zip'\n",
        "!unzip '/content/SBU/s03s02.zip'\n",
        "!unzip '/content/SBU/s03s04.zip'\n",
        "!unzip '/content/SBU/s03s05.zip'\n",
        "!unzip '/content/SBU/s03s06.zip'\n",
        "!unzip '/content/SBU/s02s01.zip'\n",
        "!unzip '/content/SBU/s02s03.zip'\n",
        "!unzip '/content/SBU/s02s06.zip'\n",
        "!unzip '/content/SBU/s02s07.zip'\n",
        "!unzip '/content/SBU/s01s03.zip'\n",
        "!unzip '/content/SBU/s01s07.zip'\n",
        "!unzip '/content/SBU/s01s02.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuJpVzL_AolT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import scipy.ndimage.interpolation as inter\n",
        "\n",
        "class SBU_dataset():\n",
        "    def __init__(self, dir):\n",
        "        print ('loading data from:', dir)\n",
        "        \n",
        "        self.pose_paths = glob.glob(os.path.join(dir, 's*', '*','*','*.txt'))\n",
        "        self.pose_paths.sort()\n",
        "        \n",
        "    \n",
        "    def get_data(self, test_set_folder):\n",
        "               \n",
        "        cross_set = {}\n",
        "        cross_set[0] = ['s01s02', 's03s04', 's05s02', 's06s04']\n",
        "        cross_set[1] = ['s02s03', 's02s07', 's03s05', 's05s03']\n",
        "        cross_set[2] = ['s01s03', 's01s07', 's07s01', 's07s03']\n",
        "        cross_set[3] = ['s02s01', 's02s06', 's03s02', 's03s06']\n",
        "        cross_set[4] = ['s04s02', 's04s03', 's04s06', 's06s02', 's06s03']\n",
        "        \n",
        "        def read_txt(pose_path):\n",
        "            a = pd.read_csv(pose_path,header=None).T\n",
        "            a = a[1:]\n",
        "            return a.as_matrix()\n",
        "        \n",
        "        print('test set folder should be slected from 0 ~ 4')\n",
        "        print('slected test folder {} includes:'.format(test_set_folder), cross_set[test_set_folder])\n",
        "\n",
        "        train_set = []\n",
        "        test_set = []\n",
        "        for i in range(len(cross_set)):\n",
        "            if i == test_set_folder:\n",
        "                test_set += cross_set[i]\n",
        "            else:\n",
        "                train_set += cross_set[i]\n",
        "\n",
        "        train = {} \n",
        "        test = {}\n",
        "\n",
        "        for i in range(1,9):\n",
        "            train[i] = []\n",
        "            test[i] = []\n",
        "\n",
        "        for pose_path in self.pose_paths:\n",
        "            pose = read_txt(pose_path)\n",
        "            if pose_path.split('/')[-4] in train_set:   \n",
        "                train[int(pose_path.split('/')[-3])].append(pose) \n",
        "            else:\n",
        "                test[int(pose_path.split('/')[-3])].append(pose) \n",
        "\n",
        "        return train, test\n",
        "        \n",
        "\n",
        "#Transfer to orginial coordinates for plotting\n",
        "def coord2org(p): \n",
        "    p_new = np.empty_like(p)\n",
        "    for i in range(15):\n",
        "        p_new[i,0] = 640 - (p[i,0] * 640)\n",
        "        p_new[i,1] = 480 - (p[i,1] * 240)\n",
        "    return p_new\n",
        "\n",
        "#Plotting the pose\n",
        "def draw_2d_pose(gtorigs): \n",
        "    f_ind = np.array([\n",
        "        [2,1,0],\n",
        "        [3,6,2,3],\n",
        "        [3,4,5],\n",
        "        [6,7,8],\n",
        "        [2,12,13,14],\n",
        "        [2,9,10,11],      \n",
        "    ])\n",
        "\n",
        "    fig = plt.figure()\n",
        "    \n",
        "    axes = plt.gca()\n",
        "    axes.set_xlim([0,640])\n",
        "    axes.set_ylim([0,480])\n",
        "\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    for gtorig,color in zip(gtorigs,['r','b']):\n",
        "        \n",
        "        gtorig = coord2org(gtorig)\n",
        "        \n",
        "        for i in range(f_ind.shape[0]):\n",
        "        \n",
        "            ax.plot(gtorig[f_ind[i], 0], gtorig[f_ind[i], 1], c=color)\n",
        "            ax.scatter(gtorig[f_ind[i], 0], gtorig[f_ind[i], 1],s=10,c=color)\n",
        "        \n",
        "    plt.show()\n",
        "\n",
        "#Rescale to be 16 frames\n",
        "def zoom(p):\n",
        "    l = p.shape[0]\n",
        "    p_new = np.empty([16,15,3]) \n",
        "    for m in range(15):\n",
        "        for n in range(3):\n",
        "            p_new[:,m,n] = inter.zoom(p[:,m,n],16/l)[:16]\n",
        "    return p_new\n",
        "\n",
        "#Switch two persons' position\n",
        "def mirror(p_0,p_1):\n",
        "    p_0_new = np.copy(p_0)\n",
        "    p_1_new = np.copy(p_1)\n",
        "    p_0_new[:,:,0] = abs(p_0_new[:,:,0]-1)\n",
        "    p_1_new[:,:,0] = abs(p_1_new[:,:,0]-1)\n",
        "    return p_0_new, p_1_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp7jeZcuA8tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.layers.core import *\n",
        "from keras.layers import *\n",
        "from keras.layers.convolutional import *\n",
        "from keras import backend as K\n",
        "from keras.optimizers import rmsprop\n",
        "import tensorflow as tf\n",
        "\n",
        "def squeeze_middle2axes_operator( x4d ) :\n",
        "    shape = tf.shape( x4d ) # get dynamic tensor shape\n",
        "    x3d = tf.reshape( x4d, [shape[0], shape[1] * shape[2], shape[3] ] )\n",
        "    return x3d\n",
        "\n",
        "def squeeze_middle2axes_shape( x4d_shape ) :\n",
        "    in_batch, in_rows, in_cols, in_filters = x4d_shape\n",
        "    if ( None in [ in_rows, in_cols] ) :\n",
        "        output_shape = ( in_batch, None, in_filters )\n",
        "    else :\n",
        "        output_shape = ( in_batch, in_rows * in_cols, in_filters )\n",
        "    return output_shape\n",
        "\n",
        "\n",
        "\n",
        "def one_obj(frame_l=16, joint_n=15, joint_d=3):\n",
        "\n",
        "    input_joints = Input(name='joints', shape=(frame_l, joint_n, joint_d))\n",
        "    input_joints_diff = Input(name='joints_diff', shape=(frame_l, joint_n, joint_d))\n",
        "    \n",
        "    ##########branch 1##############\n",
        "    x = Conv2D(filters = 32, kernel_size=(1,1),padding='same')(input_joints)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    \n",
        "    x = Conv2D(filters = 16, kernel_size=(3,1),padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Permute((1,3,2))(x)\n",
        "    \n",
        "    x = Conv2D(filters = 16, kernel_size=(3,3),padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)   \n",
        "    ##########branch 1##############\n",
        "    \n",
        "    ##########branch 2##############Temporal difference\n",
        "    x_d = Conv2D(filters = 32, kernel_size=(1,1),padding='same')(input_joints_diff)\n",
        "    x_d = BatchNormalization()(x_d)\n",
        "    x_d = LeakyReLU()(x_d)\n",
        "    \n",
        "    x_d = Conv2D(filters = 16, kernel_size=(3,1),padding='same')(x_d)\n",
        "    x_d = BatchNormalization()(x_d)\n",
        "    x_d = LeakyReLU()(x_d)\n",
        "\n",
        "    x_d = Permute((1,3,2))(x_d)\n",
        "    \n",
        "    x_d = Conv2D(filters = 16, kernel_size=(3,3),padding='same')(x_d)\n",
        "    x_d = BatchNormalization()(x_d)\n",
        "    x_d = LeakyReLU()(x_d)\n",
        "    ##########branch 2##############\n",
        "    \n",
        "    x = concatenate([x,x_d],axis=-1)\n",
        "    \n",
        "    x = Conv2D(filters = 32, kernel_size=(1,1),padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x) \n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    yy = Lambda( squeeze_middle2axes_operator, output_shape = squeeze_middle2axes_shape )(x) \n",
        "    \n",
        "    x_new = LSTM(128,activation='relu',recurrent_dropout=0.1,return_sequences=True)(yy)\n",
        "    x_new = BatchNormalization()(x_new)\n",
        "    x_new = LeakyReLU()(x_new)\n",
        "    x_new = Dropout(0.1)(x_new)\n",
        "\n",
        "    x = Conv2D(filters = 64, kernel_size=(1,1),padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x) \n",
        "    x = Dropout(0.1)(x)\n",
        "      \n",
        "    model = Model(inputs=[input_joints,input_joints_diff],outputs=[x,x_new])\n",
        "\n",
        "    return model\n",
        "\n",
        "def multi_obj(frame_l=16, joint_n=15, joint_d=3,person=1):\n",
        "    inp_j_0 = Input(name='inp_j_0', shape=(frame_l, joint_n, joint_d))\n",
        "    inp_j_diff_0 = Input(name='inp_j_diff_0', shape=(frame_l, joint_n, joint_d))\n",
        "    \n",
        "    inp_j_1 = Input(name='inp_j_1', shape=(frame_l, joint_n, joint_d))\n",
        "    inp_j_diff_1 = Input(name='inp_j_diff_1', shape=(frame_l, joint_n, joint_d))\n",
        "    \n",
        "    single = one_obj()\n",
        "    x, x_new= single([inp_j_0,inp_j_diff_0])\n",
        "    x_b, x_new_b = single([inp_j_1,inp_j_diff_1])\n",
        "    \n",
        "    x_new = Maximum()([x_new_b,x_new])\n",
        "    x = Maximum()([x_b,x])\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "     \n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    x_new = Flatten()(x_new)\n",
        "    x_new = Dropout(0.1)(x_new)\n",
        "    x_new = Dense(256)(x_new)\n",
        "    x_new = BatchNormalization()(x_new)\n",
        "    x_new = LeakyReLU()(x_new)\n",
        "    x_new = Dropout(0.1)(x_new)\n",
        "\n",
        "    x = Average()([x,x_new])\n",
        "\n",
        "    x = Dense(8, activation='sigmoid')(x)\n",
        "      \n",
        "    #model = Model(inputs=[inp_j_0,inp_j_diff_0],outputs=[x])\n",
        "    model = Model([inp_j_0,inp_j_diff_0,inp_j_1,inp_j_diff_1],x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t5UpESlEN9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shutil.move('/content/s01s02','/content/drive/My Drive/s01s02')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb3Kdtj6FbBq",
        "colab_type": "code",
        "outputId": "4f1f6313-8192-4496-d3c8-e69c88bd9e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "dataset = SBU_dataset('/content')\n",
        "train, test = dataset.get_data(3)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data from: /content\n",
            "test set folder should be slected from 0 ~ 4\n",
            "slected test folder 3 includes: ['s02s01', 's02s06', 's03s02', 's03s06']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYqhey6IJgeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SP3qAFvKP0C",
        "colab_type": "code",
        "outputId": "ae36b8dd-19f2-4284-97a0-ce2dde0801b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lr=0.001\n",
        "adam = optimizers.Adam(lr)\n",
        "model = multi_obj()\n",
        "model.compile(adam, loss='mean_squared_error')\n",
        "model.summary()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inp_j_1 (InputLayer)            (None, 16, 15, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inp_j_diff_1 (InputLayer)       (None, 16, 15, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inp_j_0 (InputLayer)            (None, 16, 15, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inp_j_diff_0 (InputLayer)       (None, 16, 15, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_16 (Model)                [(None, 4, 4, 64), ( 94720       inp_j_0[0][0]                    \n",
            "                                                                 inp_j_diff_0[0][0]               \n",
            "                                                                 inp_j_1[0][0]                    \n",
            "                                                                 inp_j_diff_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "maximum_4 (Maximum)             (None, 4, 4, 64)     0           model_16[2][0]                   \n",
            "                                                                 model_16[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "maximum_3 (Maximum)             (None, 64, 128)      0           model_16[2][1]                   \n",
            "                                                                 model_16[1][1]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 1024)         0           maximum_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (None, 8192)         0           maximum_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 1024)         0           flatten_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 8192)         0           flatten_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 256)          262400      dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 256)          2097408     dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 256)          1024        dense_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 256)          1024        dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_103 (LeakyReLU)     (None, 256)          0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_104 (LeakyReLU)     (None, 256)          0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 256)          0           leaky_re_lu_103[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 256)          0           leaky_re_lu_104[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "average_7 (Average)             (None, 256)          0           dropout_58[0][0]                 \n",
            "                                                                 dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 8)            2056        average_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,458,632\n",
            "Trainable params: 2,456,904\n",
            "Non-trainable params: 1,728\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8NNkBn8KSGd",
        "colab_type": "code",
        "outputId": "ca2b81fa-fee7-4ffa-9442-7e7e53a8645e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 300\n",
        "import random\n",
        "for e in range(epochs):\n",
        "    X_0 = []\n",
        "    X_1 = []\n",
        "    X_2 = []\n",
        "    X_3 = []\n",
        "    Y = []\n",
        "\n",
        "    for i in range(1,9):                 # loop 8 classes\n",
        "        for j in range(len(train[i])):   # loop all samples within the same class\n",
        "            \n",
        "            #First person pose\n",
        "            p_0 = np.copy(train[i][j].T[:,:45])\n",
        "            p_0 = p_0.reshape([-1,15,3])\n",
        "            t_0 = p_0.shape[0]           # the number of all frames\n",
        "            if t_0>16:                   # sample the range from crop size of [16,t_0]\n",
        "                ratio = np.random.uniform(1,t_0/16)\n",
        "                l = int(16*ratio)\n",
        "                start = random.sample(range(t_0-l),1)[0]\n",
        "                end = start+l\n",
        "                p_0 = p_0[start:end,:,:]\n",
        "                p_0 = zoom(p_0)\n",
        "            elif t_0<16:\n",
        "                p_0 = zoom(p_0)\n",
        "            \n",
        "            #Second person pose\n",
        "            p_1 = np.copy(train[i][j].T[:,45:])\n",
        "            p_1 = p_1.reshape([-1,15,3])\n",
        "            t_1 = p_1.shape[0]\n",
        "            if t_1 >16:  \n",
        "                ratio = np.random.uniform(1,t_1/16)\n",
        "                l = int(16*ratio)\n",
        "                start = random.sample(range(t_1-l),1)[0]\n",
        "                end = start+l\n",
        "                p_1 = p_1[start:end,:,:]\n",
        "                p_1 = zoom(p_1)\n",
        "            elif t_1 <16:\n",
        "                p_1 = zoom(p_1)\n",
        "            \n",
        "            # randomly mirror augmentation \n",
        "            # since two persions' postion could be switched\n",
        "            if np.random.choice([0,1],1): \n",
        "                p_0, p_1 = mirror(p_0,p_1)\n",
        "\n",
        "            \n",
        "            #Calculate the temporal difference\n",
        "            p_0_diff = p_0[1:,:,:]-p_0[:-1,:,:]\n",
        "            p_0_diff = np.concatenate((p_0_diff,np.expand_dims(p_0_diff[-1,:,:],axis=0)))\n",
        "            p_1_diff = p_1[1:,:,:]-p_1[:-1,:,:]\n",
        "            p_1_diff = np.concatenate((p_1_diff,np.expand_dims(p_1_diff[-1,:,:],axis=0)))\n",
        "\n",
        "            X_0.append(p_0)\n",
        "            X_1.append(p_0_diff)\n",
        "            X_2.append(p_1)\n",
        "            X_3.append(p_1_diff)\n",
        "\n",
        "            label = np.zeros(8)\n",
        "            label[i-1] = 1\n",
        "            Y.append(label)\n",
        "\n",
        "    X_0 = np.stack(X_0)\n",
        "    X_1 = np.stack(X_1)\n",
        "    X_2 = np.stack(X_2)\n",
        "    X_3 = np.stack(X_3)\n",
        "    Y = np.stack(Y)    \n",
        "    print('Epoch:',e)\n",
        "    history = model.fit([X_0,X_1,X_2,X_3],Y,batch_size=64,epochs=1,verbose=True,shuffle=True)\n",
        "    \n",
        "    if not (e+1)%50:\n",
        "        lr *= 0.8\n",
        "        adam = optimizers.Adam(lr)\n",
        "        model.compile(adam, loss='mean_squared_error')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 35s 163ms/step - loss: 0.2538\n",
            "Epoch: 1\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.1873\n",
            "Epoch: 2\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.1517\n",
            "Epoch: 3\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.1264\n",
            "Epoch: 4\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.1115\n",
            "Epoch: 5\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.1091\n",
            "Epoch: 6\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.1118\n",
            "Epoch: 7\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.1034\n",
            "Epoch: 8\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0887\n",
            "Epoch: 9\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0742\n",
            "Epoch: 10\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0680\n",
            "Epoch: 11\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0604\n",
            "Epoch: 12\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0548\n",
            "Epoch: 13\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0548\n",
            "Epoch: 14\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0527\n",
            "Epoch: 15\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0463\n",
            "Epoch: 16\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0496\n",
            "Epoch: 17\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0467\n",
            "Epoch: 18\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0435\n",
            "Epoch: 19\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0419\n",
            "Epoch: 20\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0453\n",
            "Epoch: 21\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0413\n",
            "Epoch: 22\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0397\n",
            "Epoch: 23\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0340\n",
            "Epoch: 24\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0357\n",
            "Epoch: 25\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0354\n",
            "Epoch: 26\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0315\n",
            "Epoch: 27\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0340\n",
            "Epoch: 28\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0327\n",
            "Epoch: 29\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0340\n",
            "Epoch: 30\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0303\n",
            "Epoch: 31\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0295\n",
            "Epoch: 32\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0294\n",
            "Epoch: 33\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0291\n",
            "Epoch: 34\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0283\n",
            "Epoch: 35\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0288\n",
            "Epoch: 36\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0266\n",
            "Epoch: 37\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0265\n",
            "Epoch: 38\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0232\n",
            "Epoch: 39\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0263\n",
            "Epoch: 40\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0258\n",
            "Epoch: 41\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0253\n",
            "Epoch: 42\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0224\n",
            "Epoch: 43\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0243\n",
            "Epoch: 44\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0167\n",
            "Epoch: 45\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0264\n",
            "Epoch: 46\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0207\n",
            "Epoch: 47\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0234\n",
            "Epoch: 48\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0234\n",
            "Epoch: 49\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0179\n",
            "Epoch: 50\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 34s 156ms/step - loss: 0.0246\n",
            "Epoch: 51\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0251\n",
            "Epoch: 52\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0232\n",
            "Epoch: 53\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0216\n",
            "Epoch: 54\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0176\n",
            "Epoch: 55\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0182\n",
            "Epoch: 56\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0167\n",
            "Epoch: 57\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0159\n",
            "Epoch: 58\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0200\n",
            "Epoch: 59\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0248\n",
            "Epoch: 60\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0218\n",
            "Epoch: 61\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0225\n",
            "Epoch: 62\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0180\n",
            "Epoch: 63\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0180\n",
            "Epoch: 64\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0194\n",
            "Epoch: 65\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0202\n",
            "Epoch: 66\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0150\n",
            "Epoch: 67\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0196\n",
            "Epoch: 68\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0185\n",
            "Epoch: 69\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0180\n",
            "Epoch: 70\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0138\n",
            "Epoch: 71\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0162\n",
            "Epoch: 72\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0190\n",
            "Epoch: 73\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0136\n",
            "Epoch: 74\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0139\n",
            "Epoch: 75\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0146\n",
            "Epoch: 76\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0163\n",
            "Epoch: 77\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0149\n",
            "Epoch: 78\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0108\n",
            "Epoch: 79\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 11ms/step - loss: 0.0116\n",
            "Epoch: 80\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0188\n",
            "Epoch: 81\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0187\n",
            "Epoch: 82\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0079\n",
            "Epoch: 83\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0120\n",
            "Epoch: 84\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0088\n",
            "Epoch: 85\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0178\n",
            "Epoch: 86\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0131\n",
            "Epoch: 87\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0165\n",
            "Epoch: 88\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0152\n",
            "Epoch: 89\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0127\n",
            "Epoch: 90\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0114\n",
            "Epoch: 91\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0111\n",
            "Epoch: 92\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0106\n",
            "Epoch: 93\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0091\n",
            "Epoch: 94\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0118\n",
            "Epoch: 95\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0098\n",
            "Epoch: 96\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0117\n",
            "Epoch: 97\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0120\n",
            "Epoch: 98\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0108\n",
            "Epoch: 99\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0098\n",
            "Epoch: 100\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 34s 159ms/step - loss: 0.0083\n",
            "Epoch: 101\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0116\n",
            "Epoch: 102\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0087\n",
            "Epoch: 103\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0055\n",
            "Epoch: 104\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0104\n",
            "Epoch: 105\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0093\n",
            "Epoch: 106\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0083\n",
            "Epoch: 107\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0066\n",
            "Epoch: 108\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0081\n",
            "Epoch: 109\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0065\n",
            "Epoch: 110\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0058\n",
            "Epoch: 111\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0063\n",
            "Epoch: 112\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0046\n",
            "Epoch: 113\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0070\n",
            "Epoch: 114\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0055\n",
            "Epoch: 115\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0093\n",
            "Epoch: 116\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0029\n",
            "Epoch: 117\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0055\n",
            "Epoch: 118\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0034\n",
            "Epoch: 119\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0071\n",
            "Epoch: 120\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0051\n",
            "Epoch: 121\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0039\n",
            "Epoch: 122\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0074\n",
            "Epoch: 123\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0046\n",
            "Epoch: 124\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0032\n",
            "Epoch: 125\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0051\n",
            "Epoch: 126\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0038\n",
            "Epoch: 127\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0074\n",
            "Epoch: 128\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0032\n",
            "Epoch: 129\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0041\n",
            "Epoch: 130\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0024\n",
            "Epoch: 131\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0022\n",
            "Epoch: 132\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0045\n",
            "Epoch: 133\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0038\n",
            "Epoch: 134\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0032\n",
            "Epoch: 135\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0034\n",
            "Epoch: 136\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0028\n",
            "Epoch: 137\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0029\n",
            "Epoch: 138\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0023\n",
            "Epoch: 139\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0043\n",
            "Epoch: 140\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0047\n",
            "Epoch: 141\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0026\n",
            "Epoch: 142\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0055\n",
            "Epoch: 143\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0061\n",
            "Epoch: 144\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0041\n",
            "Epoch: 145\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0029\n",
            "Epoch: 146\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0020\n",
            "Epoch: 147\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0028\n",
            "Epoch: 148\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0022\n",
            "Epoch: 149\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0034\n",
            "Epoch: 150\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 36s 165ms/step - loss: 0.0016\n",
            "Epoch: 151\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0032\n",
            "Epoch: 152\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0031\n",
            "Epoch: 153\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0027\n",
            "Epoch: 154\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0037\n",
            "Epoch: 155\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0020\n",
            "Epoch: 156\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0013\n",
            "Epoch: 157\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0031\n",
            "Epoch: 158\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0043\n",
            "Epoch: 159\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0047\n",
            "Epoch: 160\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0023\n",
            "Epoch: 161\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0038\n",
            "Epoch: 162\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0016\n",
            "Epoch: 163\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0036\n",
            "Epoch: 164\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0029\n",
            "Epoch: 165\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0017\n",
            "Epoch: 166\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0027\n",
            "Epoch: 167\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0012\n",
            "Epoch: 168\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0038\n",
            "Epoch: 169\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0038\n",
            "Epoch: 170\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0020\n",
            "Epoch: 171\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0017\n",
            "Epoch: 172\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0026\n",
            "Epoch: 173\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0023\n",
            "Epoch: 174\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0017\n",
            "Epoch: 175\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0014\n",
            "Epoch: 176\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0015\n",
            "Epoch: 177\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 8.4681e-04\n",
            "Epoch: 178\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0022\n",
            "Epoch: 179\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0024\n",
            "Epoch: 180\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 6.7588e-04\n",
            "Epoch: 181\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0024\n",
            "Epoch: 182\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0032\n",
            "Epoch: 183\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0044\n",
            "Epoch: 184\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0016\n",
            "Epoch: 185\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0017\n",
            "Epoch: 186\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0020\n",
            "Epoch: 187\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0025\n",
            "Epoch: 188\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0030\n",
            "Epoch: 189\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0020\n",
            "Epoch: 190\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 7.5299e-04\n",
            "Epoch: 191\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0019\n",
            "Epoch: 192\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0023\n",
            "Epoch: 193\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0043\n",
            "Epoch: 194\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0017\n",
            "Epoch: 195\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0019\n",
            "Epoch: 196\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0011\n",
            "Epoch: 197\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0026\n",
            "Epoch: 198\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0029\n",
            "Epoch: 199\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0011\n",
            "Epoch: 200\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 37s 170ms/step - loss: 0.0017\n",
            "Epoch: 201\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0017\n",
            "Epoch: 202\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0016\n",
            "Epoch: 203\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0024\n",
            "Epoch: 204\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0027\n",
            "Epoch: 205\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0028\n",
            "Epoch: 206\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0022\n",
            "Epoch: 207\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0011\n",
            "Epoch: 208\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0018\n",
            "Epoch: 209\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 9.3295e-04\n",
            "Epoch: 210\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0025\n",
            "Epoch: 211\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0033\n",
            "Epoch: 212\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 8.7043e-04\n",
            "Epoch: 213\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0012\n",
            "Epoch: 214\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 5.3079e-04\n",
            "Epoch: 215\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0011\n",
            "Epoch: 216\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0012\n",
            "Epoch: 217\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0026\n",
            "Epoch: 218\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0015\n",
            "Epoch: 219\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.0021e-04\n",
            "Epoch: 220\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0010\n",
            "Epoch: 221\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.4714e-04\n",
            "Epoch: 222\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 5.8279e-04\n",
            "Epoch: 223\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0017\n",
            "Epoch: 224\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0013\n",
            "Epoch: 225\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0014\n",
            "Epoch: 226\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.9276e-04\n",
            "Epoch: 227\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0016\n",
            "Epoch: 228\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0014\n",
            "Epoch: 229\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0012\n",
            "Epoch: 230\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0024\n",
            "Epoch: 231\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0013\n",
            "Epoch: 232\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0012\n",
            "Epoch: 233\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 6.7850e-04\n",
            "Epoch: 234\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 8.3532e-04\n",
            "Epoch: 235\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.3254e-04\n",
            "Epoch: 236\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 6.7943e-04\n",
            "Epoch: 237\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0014\n",
            "Epoch: 238\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0012\n",
            "Epoch: 239\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0023\n",
            "Epoch: 240\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0011\n",
            "Epoch: 241\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0013\n",
            "Epoch: 242\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0010\n",
            "Epoch: 243\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0014\n",
            "Epoch: 244\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 9.1327e-04\n",
            "Epoch: 245\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 9.0582e-04\n",
            "Epoch: 246\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0011\n",
            "Epoch: 247\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0024\n",
            "Epoch: 248\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0012\n",
            "Epoch: 249\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.3240e-04\n",
            "Epoch: 250\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 37s 172ms/step - loss: 0.0017\n",
            "Epoch: 251\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 5.9044e-04\n",
            "Epoch: 252\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.6893e-04\n",
            "Epoch: 253\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 8.7845e-04\n",
            "Epoch: 254\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0011\n",
            "Epoch: 255\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0010\n",
            "Epoch: 256\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0013\n",
            "Epoch: 257\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 3.9320e-04\n",
            "Epoch: 258\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 6.8590e-04\n",
            "Epoch: 259\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 3.8543e-04\n",
            "Epoch: 260\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 7.3471e-04\n",
            "Epoch: 261\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 8.7910e-04\n",
            "Epoch: 262\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 3s 12ms/step - loss: 3.3936e-04\n",
            "Epoch: 263\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0010\n",
            "Epoch: 264\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0012\n",
            "Epoch: 265\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0010\n",
            "Epoch: 266\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.9276e-04\n",
            "Epoch: 267\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.9351e-04\n",
            "Epoch: 268\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 7.2182e-04\n",
            "Epoch: 269\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 7.6466e-04\n",
            "Epoch: 270\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 7.5501e-04\n",
            "Epoch: 271\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 3.2961e-04\n",
            "Epoch: 272\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 3.8951e-04\n",
            "Epoch: 273\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0020\n",
            "Epoch: 274\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 3.0768e-04\n",
            "Epoch: 275\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 5.7621e-04\n",
            "Epoch: 276\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 3.9971e-04\n",
            "Epoch: 277\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.8109e-04\n",
            "Epoch: 278\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.9114e-04\n",
            "Epoch: 279\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 1.2019e-04\n",
            "Epoch: 280\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 6.6276e-04\n",
            "Epoch: 281\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 6.0956e-04\n",
            "Epoch: 282\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 7.3461e-04\n",
            "Epoch: 283\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0017\n",
            "Epoch: 284\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0014\n",
            "Epoch: 285\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 4.2420e-04\n",
            "Epoch: 286\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0017\n",
            "Epoch: 287\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0013\n",
            "Epoch: 288\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 7.3603e-04\n",
            "Epoch: 289\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 5.9461e-04\n",
            "Epoch: 290\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 5.2724e-04\n",
            "Epoch: 291\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 5.9392e-04\n",
            "Epoch: 292\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 2.0672e-04\n",
            "Epoch: 293\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0013\n",
            "Epoch: 294\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 8.5314e-04\n",
            "Epoch: 295\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 8.0769e-04\n",
            "Epoch: 296\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 0.0018\n",
            "Epoch: 297\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 8.1134e-04\n",
            "Epoch: 298\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 1.3428e-04\n",
            "Epoch: 299\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 2s 10ms/step - loss: 7.0239e-04\n",
            "Epoch: 300\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-cba5d89ded0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2931\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2932\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2933\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2934\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2883\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2884\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2885\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2886\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2887\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \"\"\"\n\u001b[1;32m   1504\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1458\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1460\u001b[0;31m             session._session, options_ptr)\n\u001b[0m\u001b[1;32m   1461\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWXzOWotKo2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "8dac229a-3747-4334-e89b-570ea0513df6"
      },
      "source": [
        "X_TEST_0 = []\n",
        "X_TEST_1 = []\n",
        "X_TEST_2 = []\n",
        "X_TEST_3 = []\n",
        "Y_TEST = []\n",
        "\n",
        "for i in range(1,9):\n",
        "    for j in range(len(test[i])):\n",
        "        p_0 = np.copy(test[i][j].T[:,:45])\n",
        "        p_0 = p_0.reshape([-1,15,3])\n",
        "        t_0 = p_0.shape[0]\n",
        "        # if the number of frame is more than 20, crop by scale 0.9, then rescale by interploration again\n",
        "        if t_0>=20: \n",
        "            p_0 = p_0[int(t_0*0.05):int(t_0*0.95),:,:]\n",
        "            p_0 = zoom(p_0)\n",
        "        elif t_0<20:\n",
        "            p_0 = zoom(p_0)\n",
        "        p_0_diff = p_0[1:,:,:]-p_0[:-1,:,:]\n",
        "        p_0_diff = np.concatenate((p_0_diff,np.expand_dims(p_0_diff[-1,:,:],axis=0)))\n",
        "        \n",
        "        p_1 = np.copy(test[i][j].T[:,45:])\n",
        "        p_1 = p_1.reshape([-1,15,3])\n",
        "        t_1 = p_1.shape[0]\n",
        "        if t_1 >=20:  \n",
        "            p_1 = p_1[int(t_1*0.05):int(t_1*0.95),:,:]\n",
        "            p_1 = zoom(p_1)\n",
        "        elif t_1 <20:\n",
        "            p_1 = zoom(p_1)\n",
        "        p_1_diff = p_1[1:,:,:]-p_1[:-1,:,:]\n",
        "        p_1_diff = np.concatenate((p_1_diff,np.expand_dims(p_1_diff[-1,:,:],axis=0)))\n",
        "\n",
        "        X_TEST_0.append(p_0)\n",
        "        X_TEST_1.append(p_0_diff)\n",
        "        X_TEST_2.append(p_1)\n",
        "        X_TEST_3.append(p_1_diff)\n",
        "        \n",
        "        label = np.zeros(8)\n",
        "        label[i-1] = 1\n",
        "        Y_TEST.append(label)\n",
        "\n",
        "X_TEST_0 = np.stack(X_TEST_0)\n",
        "X_TEST_1 = np.stack(X_TEST_1)\n",
        "X_TEST_2 = np.stack(X_TEST_2)\n",
        "X_TEST_3 = np.stack(X_TEST_3)\n",
        "Y_TEST = np.stack(Y_TEST)\n",
        "\n",
        "Y_pred = model.predict([X_TEST_0,X_TEST_1,X_TEST_2,X_TEST_3])\n",
        "\n",
        "print('Predict labels:',np.argmax(Y_pred,axis=1))\n",
        "print('Ground   truth:',np.argmax(Y_TEST,axis=1))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict labels: [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 4 4 4 5 5\n",
            " 5 5 1 4 6 6 6 6 6 6 7 2 7 7 7 0 7]\n",
            "Ground   truth: [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 4 4 4 5 5\n",
            " 5 5 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fxpa63whzrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}