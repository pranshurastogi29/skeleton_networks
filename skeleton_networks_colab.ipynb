{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skeleton_networks_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7J/z7koCg6CBPoJ6BXEoh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshurastogi29/skeleton_networks/blob/master/skeleton_networks_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KGRcGaqB4QN",
        "colab_type": "code",
        "outputId": "1c0439c6-f1e5-47b1-da92-13a63fb5d0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO1wnjhrCEiI",
        "colab_type": "code",
        "outputId": "29d8dbd9-0e3e-40c4-feb4-3f0418382eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "! pip install wget"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=24208dbc2c210f3dca46255b4e6c125c3d0b5d57cc3dce8a0fe889d65b23dc31\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJCRcibUBV1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('SBU')\n",
        "SBU_dir ='/content/SBU/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSKFp1E7B0WT",
        "colab_type": "code",
        "outputId": "b0f38ea6-520b-4805-8578-af94dff0a85b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "link = []\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s02.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s07.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s01.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s06.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s07.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s02.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s04.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s05.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s06.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s02.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s06.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s05s02.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s05s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s07s02.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s06s03.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s06s04.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s07s01.zip')\n",
        "link.append('http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s07s03.zip')\n",
        "\n",
        "import wget\n",
        "for i in link:\n",
        "    temp_path = SBU_dir + i.split('/')[-1]\n",
        "    url = i\n",
        "    print(url)\n",
        "    print(temp_path)\n",
        "    wget.download(url=url, out=temp_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s02.zip\n",
            "/content/SBU/s01s02.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s03.zip\n",
            "/content/SBU/s01s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s01s07.zip\n",
            "/content/SBU/s01s07.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s01.zip\n",
            "/content/SBU/s02s01.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s03.zip\n",
            "/content/SBU/s02s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s06.zip\n",
            "/content/SBU/s02s06.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s02s07.zip\n",
            "/content/SBU/s02s07.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s02.zip\n",
            "/content/SBU/s03s02.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s04.zip\n",
            "/content/SBU/s03s04.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s05.zip\n",
            "/content/SBU/s03s05.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s03s06.zip\n",
            "/content/SBU/s03s06.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s02.zip\n",
            "/content/SBU/s04s02.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s03.zip\n",
            "/content/SBU/s04s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s04s06.zip\n",
            "/content/SBU/s04s06.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s05s02.zip\n",
            "/content/SBU/s05s02.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s05s03.zip\n",
            "/content/SBU/s05s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s06s02.zip\n",
            "/content/SBU/s06s02.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s06s03.zip\n",
            "/content/SBU/s06s03.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s06s04.zip\n",
            "/content/SBU/s06s04.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s07s01.zip\n",
            "/content/SBU/s07s01.zip\n",
            "http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/s07s03.zip\n",
            "/content/SBU/s07s03.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8noWYUDB2BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/SBU/s07s01.zip'\n",
        "!unzip '/content/SBU/s07s03.zip'\n",
        "!unzip '/content/SBU/s06s04.zip'\n",
        "!unzip '/content/SBU/s06s03.zip'\n",
        "!unzip '/content/SBU/s06s02.zip'\n",
        "!unzip '/content/SBU/s05s03.zip'\n",
        "!unzip '/content/SBU/s05s02.zip'\n",
        "!unzip '/content/SBU/s04s02.zip'\n",
        "!unzip '/content/SBU/s04s03.zip'\n",
        "!unzip '/content/SBU/s04s06.zip'\n",
        "!unzip '/content/SBU/s03s02.zip'\n",
        "!unzip '/content/SBU/s03s04.zip'\n",
        "!unzip '/content/SBU/s03s05.zip'\n",
        "!unzip '/content/SBU/s03s06.zip'\n",
        "!unzip '/content/SBU/s02s01.zip'\n",
        "!unzip '/content/SBU/s02s03.zip'\n",
        "!unzip '/content/SBU/s02s06.zip'\n",
        "!unzip '/content/SBU/s02s07.zip'\n",
        "!unzip '/content/SBU/s01s03.zip'\n",
        "!unzip '/content/SBU/s01s07.zip'\n",
        "!unzip '/content/SBU/s01s02.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuJpVzL_AolT",
        "colab_type": "code",
        "outputId": "fd8f3af3-175c-4311-cb77-80403b0c51f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import scipy.ndimage.interpolation as inter\n",
        "\n",
        "class SBU_dataset():\n",
        "    def __init__(self, dir):\n",
        "        print ('loading data from:', dir)\n",
        "        \n",
        "        self.pose_paths = glob.glob(os.path.join(dir, 's*', '*','*','*.txt'))\n",
        "        self.pose_paths.sort()\n",
        "        \n",
        "    \n",
        "    def get_data(self, test_set_folder):\n",
        "               \n",
        "        cross_set = {}\n",
        "        cross_set[0] = ['s01s02', 's03s04', 's05s02', 's06s04']\n",
        "        cross_set[1] = ['ds02s03', 's02s07', 's03s05', 's05s03']\n",
        "        cross_set[2] = ['s01s03', 's01s07', 's07s01', 's07s03']\n",
        "        cross_set[3] = ['s02s01', 's02s06', 's03s02', 's03s06']\n",
        "        cross_set[4] = ['s04s02', 's04s03', 's04s06', 's06s02', 's06s03']\n",
        "        \n",
        "        def read_txt(pose_path):\n",
        "            a = pd.read_csv(pose_path,header=None).T\n",
        "            a = a[1:]\n",
        "            return a.as_matrix()\n",
        "        \n",
        "        print('test set folder should be slected from 0 ~ 4')\n",
        "        print('slected test folder {} includes:'.format(test_set_folder), cross_set[test_set_folder])\n",
        "\n",
        "        train_set = []\n",
        "        test_set = []\n",
        "        for i in range(len(cross_set)):\n",
        "            if i == test_set_folder:\n",
        "                test_set += cross_set[i]\n",
        "            else:\n",
        "                train_set += cross_set[i]\n",
        "\n",
        "        train = {} \n",
        "        test = {}\n",
        "\n",
        "        for i in range(1,9):\n",
        "            train[i] = []\n",
        "            test[i] = []\n",
        "\n",
        "        for pose_path in self.pose_paths:\n",
        "            pose = read_txt(pose_path)\n",
        "            if pose_path.split('/')[-4] in train_set:   \n",
        "                train[int(pose_path.split('/')[-3])].append(pose) \n",
        "            else:\n",
        "                test[int(pose_path.split('/')[-3])].append(pose) \n",
        "\n",
        "        return train, test\n",
        "        \n",
        "\n",
        "#Transfer to orginial coordinates for plotting\n",
        "def coord2org(p): \n",
        "    p_new = np.empty_like(p)\n",
        "    for i in range(15):\n",
        "        p_new[i,0] = 640 - (p[i,0] * 640)\n",
        "        p_new[i,1] = 480 - (p[i,1] * 240)\n",
        "    return p_new\n",
        "\n",
        "#Plotting the pose\n",
        "def draw_2d_pose(gtorigs): \n",
        "    f_ind = np.array([\n",
        "        [2,1,0],\n",
        "        [3,6,2,3],\n",
        "        [3,4,5],\n",
        "        [6,7,8],\n",
        "        [2,12,13,14],\n",
        "        [2,9,10,11],      \n",
        "    ])\n",
        "\n",
        "    fig = plt.figure()\n",
        "    \n",
        "    axes = plt.gca()\n",
        "    axes.set_xlim([0,640])\n",
        "    axes.set_ylim([0,480])\n",
        "\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    for gtorig,color in zip(gtorigs,['r','b']):\n",
        "        \n",
        "        gtorig = coord2org(gtorig)\n",
        "        \n",
        "        for i in range(f_ind.shape[0]):\n",
        "        \n",
        "            ax.plot(gtorig[f_ind[i], 0], gtorig[f_ind[i], 1], c=color)\n",
        "            ax.scatter(gtorig[f_ind[i], 0], gtorig[f_ind[i], 1],s=10,c=color)\n",
        "        \n",
        "    plt.show()\n",
        "\n",
        "#Rescale to be 16 frames\n",
        "def zoom(p):\n",
        "    l = p.shape[0]\n",
        "    p_new = np.empty([16,15,3]) \n",
        "    for m in range(15):\n",
        "        for n in range(3):\n",
        "            p_new[:,m,n] = inter.zoom(p[:,m,n],16/l)[:16]\n",
        "    return p_new\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp7jeZcuA8tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.layers.core import *\n",
        "from keras.layers.convolutional import *\n",
        "from keras import backend as K\n",
        "from keras.optimizers import rmsprop\n",
        "import tensorflow as tf\n",
        "\n",
        "def one_obj(frame_l=16, joint_n=15, joint_d=3):\n",
        "\n",
        "    input_joints = Input(name='joints', shape=(frame_l, joint_n, joint_d))\n",
        "    input_joints_diff = Input(name='joints_diff', shape=(frame_l, joint_n, joint_d))\n",
        "    \n",
        "    ##########branch 1##############\n",
        "    x = Conv2D(filters = 32, kernel_size=(1,1),padding='same')(input_joints)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    \n",
        "    x = Conv2D(filters = 16, kernel_size=(3,1),padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    x = Permute((1,3,2))(x)\n",
        "    \n",
        "    x = Conv2D(filters = 16, kernel_size=(3,3),padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)   \n",
        "    ##########branch 1##############\n",
        "    \n",
        "    ##########branch 2##############Temporal difference\n",
        "    x_d = Conv2D(filters = 32, kernel_size=(1,1),padding='same')(input_joints_diff)\n",
        "    x_d = BatchNormalization()(x_d)\n",
        "    x_d = LeakyReLU()(x_d)\n",
        "    \n",
        "    x_d = Conv2D(filters = 16, kernel_size=(3,1),padding='same')(x_d)\n",
        "    x_d = BatchNormalization()(x_d)\n",
        "    x_d = LeakyReLU()(x_d)\n",
        "\n",
        "    x_d = Permute((1,3,2))(x_d)\n",
        "    \n",
        "    x_d = Conv2D(filters = 16, kernel_size=(3,3),padding='same')(x_d)\n",
        "    x_d = BatchNormalization()(x_d)\n",
        "    x_d = LeakyReLU()(x_d)\n",
        "    ##########branch 2##############\n",
        "    \n",
        "    x = concatenate([x,x_d],axis=-1)\n",
        "    \n",
        "    x = Conv2D(filters = 32, kernel_size=(1,1),padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x) \n",
        "    x = Dropout(0.1)(x)\n",
        "       \n",
        "    x = Conv2D(filters = 64, kernel_size=(1,1),padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x) \n",
        "    x = Dropout(0.1)(x)\n",
        "      \n",
        "    model = Model([input_joints,input_joints_diff],x)\n",
        "\n",
        "    return model\n",
        "\n",
        "def multi_obj(frame_l=16, joint_n=15, joint_d=3,person=1):\n",
        "    inp_j_0 = Input(name='inp_j_0', shape=(frame_l, joint_n, joint_d))\n",
        "    inp_j_diff_0 = Input(name='inp_j_diff_0', shape=(frame_l, joint_n, joint_d))\n",
        "    \n",
        "    single = one_obj()\n",
        "    x_0 = single([inp_j_0,inp_j_diff_0])\n",
        "      \n",
        "    x = Maximum()([x_0,x_0])\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "     \n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    \n",
        "    x = Dense(8, activation='sigmoid')(x)\n",
        "      \n",
        "    model = Model([inp_j_0,inp_j_diff_0],x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t5UpESlEN9T",
        "colab_type": "code",
        "outputId": "c597605a-bb3c-4ba9-ebee-9bbad969f59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#shutil.move('/content/s01s02','/content/drive/My Drive/s01s02')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/s01s02/s01s02'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb3Kdtj6FbBq",
        "colab_type": "code",
        "outputId": "17083a17-5fd9-4ddc-cca9-5aa85e4f6679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "dataset = SBU_dataset('/content')\n",
        "train, test = dataset.get_data(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data from: /content\n",
            "test set folder should be slected from 0 ~ 4\n",
            "slected test folder 3 includes: ['s02s01', 's02s06', 's03s02', 's03s06']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYqhey6IJgeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SP3qAFvKP0C",
        "colab_type": "code",
        "outputId": "e2630c57-6300-4cba-a2a4-2c36441f546f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lr=0.001\n",
        "adam = optimizers.Adam(lr)\n",
        "model = multi_obj()\n",
        "model.compile(adam, loss='mean_squared_error')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inp_j_0 (InputLayer)            (None, 16, 15, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inp_j_diff_0 (InputLayer)       (None, 16, 15, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Model)                 (None, 4, 4, 64)     11776       inp_j_0[0][0]                    \n",
            "                                                                 inp_j_diff_0[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "maximum_1 (Maximum)             (None, 4, 4, 64)     0           model_1[1][0]                    \n",
            "                                                                 model_1[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 1024)         0           maximum_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1024)         0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          262400      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 256)          0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 256)          0           leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 8)            2056        dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 277,256\n",
            "Trainable params: 276,296\n",
            "Non-trainable params: 960\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8NNkBn8KSGd",
        "colab_type": "code",
        "outputId": "90cc191e-3660-4a34-f62b-151a86f205a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 400\n",
        "import random\n",
        "for e in range(epochs):\n",
        "    X_0 = []\n",
        "    X_1 = []\n",
        "    X_2 = []\n",
        "    X_3 = []\n",
        "    Y = []\n",
        "\n",
        "    for i in range(1,9):                 # loop 8 classes\n",
        "        for j in range(len(train[i])):   # loop all samples within the same class\n",
        "            \n",
        "            #First person pose\n",
        "            p_0 = np.copy(train[i][j].T[:,:45])\n",
        "            p_0 = p_0.reshape([-1,15,3])\n",
        "            t_0 = p_0.shape[0]           # the number of all frames\n",
        "            if t_0>16:                   # sample the range from crop size of [16,t_0]\n",
        "                ratio = np.random.uniform(1,t_0/16)\n",
        "                l = int(16*ratio)\n",
        "                start = random.sample(range(t_0-l),1)[0]\n",
        "                end = start+l\n",
        "                p_0 = p_0[start:end,:,:]\n",
        "                p_0 = zoom(p_0)\n",
        "            elif t_0<16:\n",
        "                p_0 = zoom(p_0)\n",
        "            \n",
        "            #Calculate the temporal difference\n",
        "            p_0_diff = p_0[1:,:,:]-p_0[:-1,:,:]\n",
        "            p_0_diff = np.concatenate((p_0_diff,np.expand_dims(p_0_diff[-1,:,:],axis=0)))\n",
        "            #p_1_diff = p_1[1:,:,:]-p_1[:-1,:,:]\n",
        "            #p_1_diff = np.concatenate((p_1_diff,np.expand_dims(p_1_diff[-1,:,:],axis=0)))\n",
        "\n",
        "            X_0.append(p_0)\n",
        "            X_1.append(p_0_diff)\n",
        "            #X_2.append(p_1)\n",
        "            #X_3.append(p_1_diff)\n",
        "\n",
        "            label = np.zeros(8)\n",
        "            label[i-1] = 1\n",
        "            Y.append(label)\n",
        "\n",
        "    X_0 = np.stack(X_0)\n",
        "    X_1 = np.stack(X_1)\n",
        "    #X_2 = np.stack(X_2)\n",
        "    #X_3 = np.stack(X_3)\n",
        "    Y = np.stack(Y)    \n",
        "\n",
        "    history = model.fit([X_0,X_1],Y,batch_size=32,epochs=1,verbose=True,shuffle=True)\n",
        "    \n",
        "    if not (e+1)%50:\n",
        "        lr *= 0.8\n",
        "        adam = optimizers.Adam(lr)\n",
        "        model.compile(adam, loss='mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 3s 15ms/step - loss: 0.2689\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.1987\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.1410\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.1098\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0954\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0822\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0676\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0678\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0651\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0612\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0579\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0532\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0535\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0469\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0483\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0409\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0379\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0369\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0357\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0357\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0349\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0362\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0280\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0272\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0308\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0299\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0240\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0270\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0270\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0220\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0226\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0252\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0197\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0199\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0240\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0227\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0196\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0201\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0233\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0204\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0211\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0200\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0176\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0183\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0203\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0181\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0166\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0164\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0189\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0156\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 3s 13ms/step - loss: 0.0176\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0182\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0156\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0147\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0165\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0160\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0164\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0153\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0161\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0142\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0106\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0153\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0113\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0116\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0131\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0151\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0129\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0131\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0114\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0129\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0085\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0100\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0118\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0115\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0100\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0104\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0102\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0106\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0109\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0087\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0095\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0090\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0136\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0075\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0091\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0121\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0101\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0095\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0081\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0095\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0097\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0076\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0078\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0071\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0084\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0131\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0074\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0090\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0096\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0098\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 3s 14ms/step - loss: 0.0071\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0121\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0091\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0075\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0123\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0100\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0072\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0098\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0084\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0085\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0068\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0073\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0085\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0062\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0066\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0082\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0076\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0071\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0075\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0059\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0077\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0073\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0054\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0054\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0087\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0069\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0078\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0063\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0055\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0054\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0051\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0079\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0059\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0059\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0059\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0083\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0043\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0046\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0065\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0053\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0053\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0053\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0068\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0080\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0055\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0059\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0058\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0047\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0048\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0041\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 3s 15ms/step - loss: 0.0075\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0056\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0045\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0068\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0047\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0048\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0046\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0047\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0029\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0062\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0053\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0055\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0048\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0049\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0060\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0038\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0053\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0053\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0056\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0048\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0042\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0067\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0067\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0055\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0053\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0052\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0044\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0048\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0050\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0043\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0044\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0038\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0062\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0044\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 4s 17ms/step - loss: 0.0058\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0077\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0046\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0038\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0040\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0040\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0038\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0038\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0013\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0052\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0058\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0042\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0029\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0039\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0048\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0042\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0039\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0041\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0048\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0046\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0021\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0025\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0059\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0029\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0051\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0025\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 4s 19ms/step - loss: 0.0020\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0047\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0040\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0038\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0029\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0025\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0041\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0029\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0030\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0023\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0019\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0024\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0029\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0023\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0024\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0023\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0023\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0030\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0030\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0023\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0024\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0025\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0035\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 4s 21ms/step - loss: 0.0011\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0011\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0011\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0024\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0024\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0036\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0023\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 7.1070e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0019\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 8.2639e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0019\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0021\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 7.5023e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0021\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0024\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0019\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0024\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 9.0989e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0033\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0013\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 8.1675e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 8.4573e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 3.2328e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0023\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0031\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 7.0351e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 5s 22ms/step - loss: 9.8771e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0018\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0023\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0018\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0011\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0012\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0023\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0013\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0026\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0018\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 7.1079e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0019\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0015\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0019\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0019\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0011\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 6.8643e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0010\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0027\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0030\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 7.7076e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0021\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 6.2445e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 9.5917e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0016\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 9.0584e-04\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0024\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0032\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0013\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0028\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0013\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0017\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0025\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0020\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0022\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0037\n",
            "Epoch 1/1\n",
            "216/216 [==============================] - 1s 3ms/step - loss: 0.0018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWXzOWotKo2J",
        "colab_type": "code",
        "outputId": "d3161cb8-a749-4857-b03a-d1c84544ba3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "X_TEST_0 = []\n",
        "X_TEST_1 = []\n",
        "X_TEST_2 = []\n",
        "X_TEST_3 = []\n",
        "Y_TEST = []\n",
        "\n",
        "for i in range(1,9):\n",
        "    for j in range(len(test[i])):\n",
        "        p_0 = np.copy(test[i][j].T[:,:45])\n",
        "        p_0 = p_0.reshape([-1,15,3])\n",
        "        t_0 = p_0.shape[0]\n",
        "        # if the number of frame is more than 20, crop by scale 0.9, then rescale by interploration again\n",
        "        if t_0>=20: \n",
        "            p_0 = p_0[int(t_0*0.05):int(t_0*0.95),:,:]\n",
        "            p_0 = zoom(p_0)\n",
        "        elif t_0<20:\n",
        "            p_0 = zoom(p_0)\n",
        "        p_0_diff = p_0[1:,:,:]-p_0[:-1,:,:]\n",
        "        p_0_diff = np.concatenate((p_0_diff,np.expand_dims(p_0_diff[-1,:,:],axis=0)))\n",
        "        \n",
        "        X_TEST_0.append(p_0)\n",
        "        X_TEST_1.append(p_0_diff)\n",
        "        \n",
        "        label = np.zeros(8)\n",
        "        label[i-1] = 1\n",
        "        Y_TEST.append(label)\n",
        "\n",
        "X_TEST_0 = np.stack(X_TEST_0)\n",
        "X_TEST_1 = np.stack(X_TEST_1)\n",
        "Y_TEST = np.stack(Y_TEST)\n",
        "\n",
        "Y_pred = model.predict([X_TEST_0,X_TEST_1])\n",
        "\n",
        "print('Predict labels:',np.argmax(Y_pred,axis=1))\n",
        "print('Ground truth labels:',np.argmax(Y_TEST,axis=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict labels: [0 7 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 2 7 2 2 2 2 1 2 2 2 7 3 7 3 1\n",
            " 3 3 6 4 4 4 5 5 5 3 7 4 4 6 6 0 0 6 3 6 4 7 2 3 1 7 7 7 7]\n",
            "Ground truth labels: [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3\n",
            " 3 3 3 4 4 4 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fxpa63whzrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
